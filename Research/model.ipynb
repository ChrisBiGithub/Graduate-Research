{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compliation tracking\n",
    "1. Preliminary model with $ 2 (14-12-2) $ layers, **regular** cross entropy loss function, no dropout <br>\n",
    "&emsp;1.1 Features: origninal proposed features, no kyg features, min-max normalization for the features <br>\n",
    "&emsp;1.2 Algorithm from kipf & welling with localized $ k = 1$\n",
    "2. **Effective number based class-balanced weight** loss function, $2 (24-12-2)$ layers, no dropout <br>\n",
    "&emsp;2.1 Same as 1.1 <br>\n",
    "&emsp;2.2 Same as 1.2 <br>\n",
    "&emsp;2.3 Hyperarameter of the loss function is set to 0.999999\n",
    "3. **Effective number based class-balanced weight** loss function, $3 (24-16-8-2)$ layers, no dropout <br>\n",
    "4. **Effective number based class-balanced weight** loss function, $4 (24-16-16-8-2)$ layers, no dropout ,br.\n",
    "5. $K = 2, 3, 4$ instead of $1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: nvidia-smi: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Model, only using 2xzn for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PCM import PCM\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMINO_ACIDS = np.array(['ALA', 'CYS', 'ASP', 'GLU', 'PHE', 'GLY', 'HIS', 'ILE', 'LYS', 'LEU', 'MET', 'ASN', 'PRO', 'GLN', 'ARG'\n",
    "               , 'SER','THR', 'VAL', 'TRP', 'TYR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Layers of graph convolution\n",
    "\"\"\"\n",
    "\n",
    "class GClayer(Module):\n",
    "    def __init__(self, nin, nout, bias = False):\n",
    "        super(GClayer, self).__init__()\n",
    "        self.in_features = nin\n",
    "        self.out_features = nout\n",
    "        self.weight = Parameter(torch.randn(nin, nout))\n",
    "        \n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.randn(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        \n",
    "    def forward(self, input, adj):\n",
    "        temp = torch.mm(input, self.weight)\n",
    "        output = torch.sparse.mm(adj, temp)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GCN model\n",
    "\"\"\"\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid1, nhid2, nhid3, nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        \n",
    "        self.gc1 = GClayer(nfeat, nhid1)\n",
    "        self.gc2 = GClayer(nhid1, nhid2)\n",
    "        self.gc3 = GClayer(nhid2, nhid3)\n",
    "        self.gc4 = GClayer(nhid3, nclass)\n",
    "        self.dropout = dropout\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        x = F.leaky_relu(self.gc1(x, adj))\n",
    "        #x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.leaky_relu(self.gc2(x, adj))\n",
    "        x = F.leaky_relu(self.gc3(x, adj))\n",
    "        x = F.leaky_relu(self.gc4(x, adj))\n",
    "        return F.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Get all different contact map\"\"\"\n",
    "pcm = PCM('2BU1', '../datasets/Tongji/benchmark/pdb/2BU1.pdb')\n",
    "\n",
    "cm1, res_list = pcm.getContactMap1()\n",
    "cm2, _ = pcm.getContactMap2()\n",
    "cm3, _ = pcm.getContactMap3()\n",
    "cm4, _ = pcm.getContactMap4()\n",
    "\n",
    "#different contact map's adjacency matrix\n",
    "adj1 = sp.coo_matrix(cm1, dtype = 'int')\n",
    "adj2 = sp.coo_matrix(cm2, dtype = 'int')\n",
    "adj3 = sp.coo_matrix(cm3, dtype = 'int')\n",
    "adj4 = sp.coo_matrix(cm4, dtype = 'int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Laplacian Matrix with k = 2, 3, 4'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Laplacian Matrix with k = 1\"\"\"\n",
    "def normalize(mx):\n",
    "    row_sum = np.array(mx.sum(1), dtype = np.float32)\n",
    "    r_inv = np.power(row_sum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0\n",
    "    r_inv_mat = sp.diags(r_inv)\n",
    "    #norm_lap = sp.eye(adj.shape[0]) - r_inv_mat.dot(adj)\n",
    "    # this is wrong, the paper did not used laplacian graph\n",
    "    norm_lap = r_inv_mat.dot(mx)\n",
    "    return norm_lap\n",
    "\n",
    "# print(\"Inverse degree matrix: {}\\n Normalized Laplacian: {}\".format(r_inv_mat, norm_lap))\n",
    "\n",
    "\"\"\"Laplacian Matrix with k = 2, 3, 4\"\"\"\n",
    "# k2_lap = sp.coo_matrix(2 * (norm_lap**2) - np.eye(norm_lap.shape[0]), dtype = 'float32')\n",
    "# k3_lap = sp.coo_matrix(4 * (norm_lap**3) - 3 * norm_lap, dtype = 'float32')\n",
    "# k4_lap = sp.coo_matrix(8 * (norm_lap**4) - 8 * (norm_lap ** 2), dtype = 'float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_cm_1 = normalize(adj1)\n",
    "norm_cm_2 = normalize(adj2)\n",
    "norm_cm_3 = normalize(adj3)\n",
    "norm_cm_4 = normalize(adj4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def loadData():\"\"\"\n",
    "f = np.genfromtxt('./trial_data/2BU1.txt', dtype = np.dtype(str))\n",
    "\n",
    "features = np.array(f[:, 1:-1], dtype = np.float32)\n",
    "labels = np.array(f[:, -1], dtype = np.int_)\n",
    "reses = f[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min-max normalization for the features (bond to change a little)\n",
    "for i in range(20, 24):\n",
    "    features[:, i] = (features[:, i] - np.min(features[:, i])) / (np.max(features[:, i]) - np.min(features[:, i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Converting sparse matrix to torch tensor\n",
    "\"\"\"\n",
    "def sparseToTensor(sp_mx):\n",
    "    sp_mx = sp_mx.tocoo().astype(np.float32)\n",
    "    #vertically stack row and column indices with non-zero value\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sp_mx.row, sp_mx.col)).astype(np.int_)) \n",
    "    values = torch.from_numpy(sp_mx.data)\n",
    "    shape = torch.Size(sp_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Turn everything into tensors (part of loadData())\"\"\"\n",
    "temp = torch.LongTensor(range(adj1.shape[0]))\n",
    "temp_perm = temp[torch.randperm(adj1.shape[0])]\n",
    "\n",
    "train_idx_1 = temp_perm[:250]\n",
    "val_idx_1 = temp_perm[250:300]\n",
    "test_idx_1 = temp_perm[300:]\n",
    "\n",
    "# print(\"Train index: {} \\n Validation index: {} \\n Test index: {}\\n\".format(train_idx_1, val_idx_1, test_idx_1))\n",
    "# print(\"=========================================================================================================\")\n",
    "\n",
    "# norm_lap_1 = sparseToTensor(norm_lap)\n",
    "# k2_lap_1 = sparseToTensor(k2_lap)\n",
    "# k3_lap_1 = sparseToTensor(k3_lap)\n",
    "# k4_lap_1 = sparseToTensor(k4_lap)\n",
    "features_1 = torch.FloatTensor(features)\n",
    "labels_1 = torch.LongTensor(labels)\n",
    "\n",
    "# tensor of contact maps\n",
    "cm1_lap_1 = sparseToTensor(norm_cm_1)\n",
    "cm1_lap_2 = sparseToTensor(norm_cm_2)\n",
    "cm1_lap_3 = sparseToTensor(norm_cm_3)\n",
    "cm1_lap_4 = sparseToTensor(norm_cm_4)\n",
    "\n",
    "# print(\"Labels: {}\\n Features: {}\\n Laplacia: {}\".format(labels_1, features_1, norm_lap_1.to_dense())) #to_dense() shows the sparse matrix in regular dense format\n",
    "#return adj, features, labels, train_idx, val_idx, test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (gc1): GClayer(24 -> 16)\n",
      "  (gc2): GClayer(16 -> 16)\n",
      "  (gc3): GClayer(16 -> 8)\n",
      "  (gc4): GClayer(8 -> 2)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = GCN(nfeat = features.shape[1],\n",
    "            nhid1 = 16,\n",
    "            nhid2 = 16,\n",
    "            nhid3 = 8,\n",
    "            nclass= 2,\n",
    "            dropout = 0.5)\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr = 0.01, weight_decay = 5e-4)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.to(device)\n",
    "    features_1 = features_1.cuda() \n",
    "    norm_lap_1 = norm_lap_1.cuda()\n",
    "    k2_lap_1 = k2_lap_1.cuda()\n",
    "    labels_1 = labels_1.cuda()\n",
    "    train_idx_1 = train_idx_1.cuda()\n",
    "    val_idx_1 = val_idx_1.cuda()\n",
    "    test_idx_1 = test_idx_1.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train version #1, with flat loss function\n",
    "def train_v1(epoch, _):\n",
    "    t = time.time()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features_1, norm_lap_1)\n",
    "    \n",
    "    loss_train = criterion(output[train_idx_1], labels_1[train_idx_1])\n",
    "    acc_train = accuracy(output[train_idx_1], labels_1[train_idx_1])\n",
    "    optimizer.zero_grad()\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_val = criterion(output[val_idx_1], labels_1[val_idx_1])\n",
    "    acc_val = accuracy(output[val_idx_1], labels_1[val_idx_1])\n",
    "    \n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "    \n",
    "    return loss_train, acc_train, loss_val, acc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_sizes = np.array([labels_1.size()[0] - labels_1.nonzero().size()[0], labels_1.nonzero().size()[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train version #2, with weighted loss function, beta is a hyper parameter\n",
    "# have to try different betas\n",
    "# betas = [0.9, 0.99, 0.999, 0.9999, 0.99999, 0.999999, 0.99999999]\n",
    "def train_v2(epoch, beta):\n",
    "    ######calculate the weights based on the paper with effective number######\n",
    "    no_of_classes = 2\n",
    "    effective_num = 1.0 - np.power(beta, classes_sizes)\n",
    "    weights = (1.0 - beta) / np.array(effective_num)\n",
    "    weights = weights / np.sum(weights) * no_of_classes\n",
    "    l = torch.tensor([0, 1])\n",
    "    labels_one_hot = F.one_hot(l, no_of_classes).float()\n",
    "    weights = torch.tensor(weights).float()\n",
    "    if epoch == 0: print(weights)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(weight = weights)\n",
    "    ###########################################################\n",
    "    \n",
    "    t = time.time()\n",
    "\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features_1, cm1_lap_4)\n",
    "    \n",
    "    loss_train = criterion(output[train_idx_1], labels_1[train_idx_1])\n",
    "    acc_train = accuracy(output[train_idx_1], labels_1[train_idx_1])\n",
    "    optimizer.zero_grad()\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_val = criterion(output[val_idx_1], labels_1[val_idx_1])\n",
    "    acc_val = accuracy(output[val_idx_1], labels_1[val_idx_1])\n",
    "    \n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "    \n",
    "    return loss_train, acc_train, loss_val, acc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train version #3, with localization set to k = 2\n",
    "# four layers\n",
    "def train_v3(epoch, beta):\n",
    "    ######calculate the weights based on the paper with effective number######\n",
    "    no_of_classes = 2\n",
    "    effective_num = 1.0 - np.power(beta, classes_sizes)\n",
    "    weights = (1.0 - beta) / np.array(effective_num)\n",
    "    weights = weights / np.sum(weights) * no_of_classes\n",
    "    l = torch.tensor([0, 1])\n",
    "    labels_one_hot = F.one_hot(l, no_of_classes).float()\n",
    "    weights = torch.tensor(weights).float()\n",
    "    if epoch == 0: print(weights)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(weight = weights)\n",
    "    ###########################################################\n",
    "    \n",
    "    t = time.time()\n",
    "\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features_1, k2_lap_1)\n",
    "    \n",
    "    loss_train = criterion(output[train_idx_1], labels_1[train_idx_1])\n",
    "    acc_train = accuracy(output[train_idx_1], labels_1[train_idx_1])\n",
    "    optimizer.zero_grad()\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_val = criterion(output[val_idx_1], labels_1[val_idx_1])\n",
    "    acc_val = accuracy(output[val_idx_1], labels_1[val_idx_1])\n",
    "    \n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "    \n",
    "    return loss_train, acc_train, loss_val, acc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1344, 1.8656])\n",
      "Epoch: 0001 loss_train: 0.8100 acc_train: 0.0680 loss_val: 0.8434 acc_val: 0.0600 time: 0.0072s\n",
      "Epoch: 0002 loss_train: 0.8100 acc_train: 0.0680 loss_val: 0.8434 acc_val: 0.0600 time: 0.0039s\n",
      "Epoch: 0003 loss_train: 0.8100 acc_train: 0.0680 loss_val: 0.8434 acc_val: 0.0600 time: 0.0032s\n",
      "Epoch: 0004 loss_train: 0.8100 acc_train: 0.0680 loss_val: 0.8433 acc_val: 0.0600 time: 0.0035s\n",
      "Epoch: 0005 loss_train: 0.8099 acc_train: 0.0680 loss_val: 0.8431 acc_val: 0.0600 time: 0.0032s\n",
      "Epoch: 0006 loss_train: 0.8096 acc_train: 0.0680 loss_val: 0.8426 acc_val: 0.0600 time: 0.0029s\n",
      "Epoch: 0007 loss_train: 0.8090 acc_train: 0.0680 loss_val: 0.8406 acc_val: 0.0600 time: 0.0035s\n",
      "Epoch: 0008 loss_train: 0.8061 acc_train: 0.0680 loss_val: 0.8340 acc_val: 0.0800 time: 0.0035s\n",
      "Epoch: 0009 loss_train: 0.7996 acc_train: 0.0840 loss_val: 0.8252 acc_val: 0.0800 time: 0.0031s\n",
      "Epoch: 0010 loss_train: 0.7900 acc_train: 0.0920 loss_val: 0.8142 acc_val: 0.1200 time: 0.0030s\n",
      "Epoch: 0011 loss_train: 0.7736 acc_train: 0.1200 loss_val: 0.8066 acc_val: 0.1200 time: 0.0037s\n",
      "Epoch: 0012 loss_train: 0.7439 acc_train: 0.1680 loss_val: 0.7901 acc_val: 0.1200 time: 0.0037s\n",
      "Epoch: 0013 loss_train: 0.6929 acc_train: 0.2520 loss_val: 0.7586 acc_val: 0.2000 time: 0.0050s\n",
      "Epoch: 0014 loss_train: 0.6156 acc_train: 0.3920 loss_val: 0.7014 acc_val: 0.2200 time: 0.0036s\n",
      "Epoch: 0015 loss_train: 0.5517 acc_train: 0.6040 loss_val: 0.6002 acc_val: 0.4600 time: 0.0035s\n",
      "Epoch: 0016 loss_train: 0.5061 acc_train: 0.7320 loss_val: 0.5184 acc_val: 0.6200 time: 0.0033s\n",
      "Epoch: 0017 loss_train: 0.4857 acc_train: 0.7920 loss_val: 0.4790 acc_val: 0.7000 time: 0.0030s\n",
      "Epoch: 0018 loss_train: 0.4840 acc_train: 0.8320 loss_val: 0.4923 acc_val: 0.7600 time: 0.0028s\n",
      "Epoch: 0019 loss_train: 0.4993 acc_train: 0.8840 loss_val: 0.5374 acc_val: 0.7800 time: 0.0028s\n",
      "Epoch: 0020 loss_train: 0.5146 acc_train: 0.8880 loss_val: 0.5467 acc_val: 0.8200 time: 0.0029s\n",
      "Epoch: 0021 loss_train: 0.5172 acc_train: 0.8880 loss_val: 0.5485 acc_val: 0.8200 time: 0.0034s\n",
      "Epoch: 0022 loss_train: 0.5107 acc_train: 0.8880 loss_val: 0.5496 acc_val: 0.8200 time: 0.0031s\n",
      "Epoch: 0023 loss_train: 0.4989 acc_train: 0.8880 loss_val: 0.5487 acc_val: 0.7800 time: 0.0028s\n",
      "Epoch: 0024 loss_train: 0.4889 acc_train: 0.8840 loss_val: 0.5382 acc_val: 0.7800 time: 0.0028s\n",
      "Epoch: 0025 loss_train: 0.4816 acc_train: 0.8640 loss_val: 0.5150 acc_val: 0.7800 time: 0.0030s\n",
      "Epoch: 0026 loss_train: 0.4757 acc_train: 0.8360 loss_val: 0.4856 acc_val: 0.7600 time: 0.0028s\n",
      "Epoch: 0027 loss_train: 0.4717 acc_train: 0.8360 loss_val: 0.4677 acc_val: 0.7600 time: 0.0028s\n",
      "Epoch: 0028 loss_train: 0.4686 acc_train: 0.8280 loss_val: 0.4632 acc_val: 0.7400 time: 0.0027s\n",
      "Epoch: 0029 loss_train: 0.4679 acc_train: 0.8160 loss_val: 0.4667 acc_val: 0.7200 time: 0.0028s\n",
      "Epoch: 0030 loss_train: 0.4685 acc_train: 0.8160 loss_val: 0.4693 acc_val: 0.7200 time: 0.0025s\n",
      "Epoch: 0031 loss_train: 0.4697 acc_train: 0.8160 loss_val: 0.4706 acc_val: 0.7200 time: 0.0027s\n",
      "Epoch: 0032 loss_train: 0.4703 acc_train: 0.8160 loss_val: 0.4714 acc_val: 0.7200 time: 0.0029s\n",
      "Epoch: 0033 loss_train: 0.4703 acc_train: 0.8160 loss_val: 0.4717 acc_val: 0.7200 time: 0.0027s\n",
      "Epoch: 0034 loss_train: 0.4694 acc_train: 0.8160 loss_val: 0.4712 acc_val: 0.7200 time: 0.0027s\n",
      "Epoch: 0035 loss_train: 0.4677 acc_train: 0.8160 loss_val: 0.4698 acc_val: 0.7200 time: 0.0035s\n",
      "Epoch: 0036 loss_train: 0.4651 acc_train: 0.8160 loss_val: 0.4677 acc_val: 0.7200 time: 0.0033s\n",
      "Epoch: 0037 loss_train: 0.4617 acc_train: 0.8280 loss_val: 0.4649 acc_val: 0.7200 time: 0.0029s\n",
      "Epoch: 0038 loss_train: 0.4577 acc_train: 0.8280 loss_val: 0.4608 acc_val: 0.7400 time: 0.0031s\n",
      "Epoch: 0039 loss_train: 0.4539 acc_train: 0.8400 loss_val: 0.4567 acc_val: 0.7600 time: 0.0029s\n",
      "Epoch: 0040 loss_train: 0.4508 acc_train: 0.8480 loss_val: 0.4537 acc_val: 0.7600 time: 0.0032s\n",
      "Epoch: 0041 loss_train: 0.4488 acc_train: 0.8600 loss_val: 0.4508 acc_val: 0.7600 time: 0.0031s\n",
      "Epoch: 0042 loss_train: 0.4484 acc_train: 0.8640 loss_val: 0.4497 acc_val: 0.7600 time: 0.0032s\n",
      "Epoch: 0043 loss_train: 0.4494 acc_train: 0.8800 loss_val: 0.4501 acc_val: 0.7800 time: 0.0032s\n",
      "Epoch: 0044 loss_train: 0.4503 acc_train: 0.8920 loss_val: 0.4497 acc_val: 0.7800 time: 0.0040s\n",
      "Epoch: 0045 loss_train: 0.4492 acc_train: 0.8960 loss_val: 0.4475 acc_val: 0.7800 time: 0.0033s\n",
      "Epoch: 0046 loss_train: 0.4464 acc_train: 0.8880 loss_val: 0.4450 acc_val: 0.7800 time: 0.0029s\n",
      "Epoch: 0047 loss_train: 0.4434 acc_train: 0.8720 loss_val: 0.4440 acc_val: 0.7600 time: 0.0035s\n",
      "Epoch: 0048 loss_train: 0.4417 acc_train: 0.8720 loss_val: 0.4452 acc_val: 0.7600 time: 0.0030s\n",
      "Epoch: 0049 loss_train: 0.4413 acc_train: 0.8720 loss_val: 0.4468 acc_val: 0.7600 time: 0.0027s\n",
      "Epoch: 0050 loss_train: 0.4413 acc_train: 0.8680 loss_val: 0.4477 acc_val: 0.7600 time: 0.0026s\n",
      "Epoch: 0051 loss_train: 0.4411 acc_train: 0.8680 loss_val: 0.4481 acc_val: 0.7600 time: 0.0034s\n",
      "Epoch: 0052 loss_train: 0.4403 acc_train: 0.8720 loss_val: 0.4476 acc_val: 0.7600 time: 0.0033s\n",
      "Epoch: 0053 loss_train: 0.4388 acc_train: 0.8720 loss_val: 0.4460 acc_val: 0.7600 time: 0.0032s\n",
      "Epoch: 0054 loss_train: 0.4369 acc_train: 0.8760 loss_val: 0.4430 acc_val: 0.7600 time: 0.0042s\n",
      "Epoch: 0055 loss_train: 0.4354 acc_train: 0.8800 loss_val: 0.4387 acc_val: 0.7600 time: 0.0034s\n",
      "Epoch: 0056 loss_train: 0.4349 acc_train: 0.8920 loss_val: 0.4343 acc_val: 0.7800 time: 0.0043s\n",
      "Epoch: 0057 loss_train: 0.4355 acc_train: 0.8960 loss_val: 0.4309 acc_val: 0.7800 time: 0.0035s\n",
      "Epoch: 0058 loss_train: 0.4356 acc_train: 0.8960 loss_val: 0.4296 acc_val: 0.7800 time: 0.0032s\n",
      "Epoch: 0059 loss_train: 0.4345 acc_train: 0.8960 loss_val: 0.4308 acc_val: 0.7800 time: 0.0032s\n",
      "Epoch: 0060 loss_train: 0.4332 acc_train: 0.8920 loss_val: 0.4333 acc_val: 0.7800 time: 0.0040s\n",
      "Epoch: 0061 loss_train: 0.4327 acc_train: 0.8880 loss_val: 0.4359 acc_val: 0.7800 time: 0.0034s\n",
      "Epoch: 0062 loss_train: 0.4329 acc_train: 0.8840 loss_val: 0.4379 acc_val: 0.7800 time: 0.0034s\n",
      "Epoch: 0063 loss_train: 0.4332 acc_train: 0.8840 loss_val: 0.4388 acc_val: 0.7800 time: 0.0030s\n",
      "Epoch: 0064 loss_train: 0.4331 acc_train: 0.8840 loss_val: 0.4388 acc_val: 0.7800 time: 0.0033s\n",
      "Epoch: 0065 loss_train: 0.4326 acc_train: 0.8840 loss_val: 0.4379 acc_val: 0.7800 time: 0.0042s\n",
      "Epoch: 0066 loss_train: 0.4319 acc_train: 0.8880 loss_val: 0.4364 acc_val: 0.7800 time: 0.0031s\n",
      "Epoch: 0067 loss_train: 0.4315 acc_train: 0.8880 loss_val: 0.4345 acc_val: 0.7800 time: 0.0033s\n",
      "Epoch: 0068 loss_train: 0.4316 acc_train: 0.8920 loss_val: 0.4327 acc_val: 0.7800 time: 0.0034s\n",
      "Epoch: 0069 loss_train: 0.4317 acc_train: 0.8920 loss_val: 0.4315 acc_val: 0.7800 time: 0.0030s\n",
      "Epoch: 0070 loss_train: 0.4315 acc_train: 0.8920 loss_val: 0.4315 acc_val: 0.7800 time: 0.0025s\n",
      "Epoch: 0071 loss_train: 0.4310 acc_train: 0.8920 loss_val: 0.4326 acc_val: 0.7800 time: 0.0028s\n",
      "Epoch: 0072 loss_train: 0.4307 acc_train: 0.8880 loss_val: 0.4339 acc_val: 0.7800 time: 0.0029s\n",
      "Epoch: 0073 loss_train: 0.4306 acc_train: 0.8880 loss_val: 0.4351 acc_val: 0.7800 time: 0.0026s\n",
      "Epoch: 0074 loss_train: 0.4306 acc_train: 0.8880 loss_val: 0.4359 acc_val: 0.7800 time: 0.0028s\n",
      "Epoch: 0075 loss_train: 0.4305 acc_train: 0.8880 loss_val: 0.4361 acc_val: 0.7800 time: 0.0028s\n",
      "Epoch: 0076 loss_train: 0.4303 acc_train: 0.8880 loss_val: 0.4359 acc_val: 0.7800 time: 0.0027s\n",
      "Epoch: 0077 loss_train: 0.4300 acc_train: 0.8880 loss_val: 0.4353 acc_val: 0.7800 time: 0.0029s\n",
      "Epoch: 0078 loss_train: 0.4298 acc_train: 0.8880 loss_val: 0.4345 acc_val: 0.7800 time: 0.0028s\n",
      "Epoch: 0079 loss_train: 0.4296 acc_train: 0.8920 loss_val: 0.4336 acc_val: 0.7800 time: 0.0027s\n",
      "Epoch: 0080 loss_train: 0.4295 acc_train: 0.8920 loss_val: 0.4329 acc_val: 0.7800 time: 0.0032s\n",
      "Epoch: 0081 loss_train: 0.4295 acc_train: 0.8920 loss_val: 0.4326 acc_val: 0.7800 time: 0.0027s\n",
      "Epoch: 0082 loss_train: 0.4293 acc_train: 0.8920 loss_val: 0.4328 acc_val: 0.7800 time: 0.0029s\n",
      "Epoch: 0083 loss_train: 0.4290 acc_train: 0.8920 loss_val: 0.4334 acc_val: 0.7800 time: 0.0028s\n",
      "Epoch: 0084 loss_train: 0.4289 acc_train: 0.8920 loss_val: 0.4340 acc_val: 0.7800 time: 0.0029s\n",
      "Epoch: 0085 loss_train: 0.4288 acc_train: 0.8920 loss_val: 0.4345 acc_val: 0.7800 time: 0.0025s\n",
      "Epoch: 0086 loss_train: 0.4287 acc_train: 0.8920 loss_val: 0.4348 acc_val: 0.7800 time: 0.0024s\n",
      "Epoch: 0087 loss_train: 0.4286 acc_train: 0.8920 loss_val: 0.4348 acc_val: 0.7800 time: 0.0026s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0088 loss_train: 0.4284 acc_train: 0.8920 loss_val: 0.4345 acc_val: 0.7800 time: 0.0031s\n",
      "Epoch: 0089 loss_train: 0.4283 acc_train: 0.8920 loss_val: 0.4340 acc_val: 0.7800 time: 0.0032s\n",
      "Epoch: 0090 loss_train: 0.4281 acc_train: 0.8920 loss_val: 0.4334 acc_val: 0.7800 time: 0.0031s\n",
      "Epoch: 0091 loss_train: 0.4280 acc_train: 0.8920 loss_val: 0.4330 acc_val: 0.7800 time: 0.0030s\n",
      "Epoch: 0092 loss_train: 0.4279 acc_train: 0.8920 loss_val: 0.4328 acc_val: 0.7800 time: 0.0029s\n",
      "Epoch: 0093 loss_train: 0.4278 acc_train: 0.8920 loss_val: 0.4328 acc_val: 0.7800 time: 0.0029s\n",
      "Epoch: 0094 loss_train: 0.4276 acc_train: 0.8920 loss_val: 0.4330 acc_val: 0.7800 time: 0.0032s\n",
      "Epoch: 0095 loss_train: 0.4275 acc_train: 0.8920 loss_val: 0.4334 acc_val: 0.7800 time: 0.0031s\n",
      "Epoch: 0096 loss_train: 0.4274 acc_train: 0.8920 loss_val: 0.4336 acc_val: 0.7800 time: 0.0039s\n",
      "Epoch: 0097 loss_train: 0.4273 acc_train: 0.8920 loss_val: 0.4338 acc_val: 0.7800 time: 0.0035s\n",
      "Epoch: 0098 loss_train: 0.4272 acc_train: 0.8920 loss_val: 0.4337 acc_val: 0.7800 time: 0.0032s\n",
      "Epoch: 0099 loss_train: 0.4271 acc_train: 0.8920 loss_val: 0.4335 acc_val: 0.7800 time: 0.0034s\n",
      "Epoch: 0100 loss_train: 0.4270 acc_train: 0.8920 loss_val: 0.4331 acc_val: 0.7800 time: 0.0031s\n",
      "Epoch: 0101 loss_train: 0.4268 acc_train: 0.8920 loss_val: 0.4327 acc_val: 0.7800 time: 0.0030s\n",
      "Epoch: 0102 loss_train: 0.4267 acc_train: 0.8920 loss_val: 0.4324 acc_val: 0.7800 time: 0.0034s\n",
      "Epoch: 0103 loss_train: 0.4266 acc_train: 0.8960 loss_val: 0.4322 acc_val: 0.7800 time: 0.0031s\n",
      "Epoch: 0104 loss_train: 0.4265 acc_train: 0.8960 loss_val: 0.4322 acc_val: 0.7800 time: 0.0057s\n",
      "Epoch: 0105 loss_train: 0.4264 acc_train: 0.8960 loss_val: 0.4323 acc_val: 0.7800 time: 0.0032s\n",
      "Epoch: 0106 loss_train: 0.4263 acc_train: 0.8960 loss_val: 0.4324 acc_val: 0.7800 time: 0.0030s\n",
      "Epoch: 0107 loss_train: 0.4262 acc_train: 0.8960 loss_val: 0.4326 acc_val: 0.7800 time: 0.0030s\n",
      "Epoch: 0108 loss_train: 0.4260 acc_train: 0.8960 loss_val: 0.4326 acc_val: 0.7800 time: 0.0031s\n",
      "Epoch: 0109 loss_train: 0.4259 acc_train: 0.8960 loss_val: 0.4325 acc_val: 0.7800 time: 0.0030s\n",
      "Epoch: 0110 loss_train: 0.4258 acc_train: 0.8960 loss_val: 0.4323 acc_val: 0.7800 time: 0.0030s\n",
      "Epoch: 0111 loss_train: 0.4257 acc_train: 0.8960 loss_val: 0.4320 acc_val: 0.7800 time: 0.0029s\n",
      "Epoch: 0112 loss_train: 0.4256 acc_train: 0.8960 loss_val: 0.4317 acc_val: 0.7800 time: 0.0032s\n",
      "Epoch: 0113 loss_train: 0.4255 acc_train: 0.8960 loss_val: 0.4316 acc_val: 0.7800 time: 0.0033s\n",
      "Epoch: 0114 loss_train: 0.4254 acc_train: 0.8960 loss_val: 0.4318 acc_val: 0.7800 time: 0.0033s\n",
      "Epoch: 0115 loss_train: 0.4253 acc_train: 0.8960 loss_val: 0.4321 acc_val: 0.7800 time: 0.0030s\n",
      "Epoch: 0116 loss_train: 0.4252 acc_train: 0.8960 loss_val: 0.4323 acc_val: 0.7800 time: 0.0029s\n",
      "Epoch: 0117 loss_train: 0.4251 acc_train: 0.8960 loss_val: 0.4322 acc_val: 0.7800 time: 0.0031s\n",
      "Epoch: 0118 loss_train: 0.4250 acc_train: 0.8960 loss_val: 0.4320 acc_val: 0.7800 time: 0.0037s\n",
      "Epoch: 0119 loss_train: 0.4249 acc_train: 0.8960 loss_val: 0.4318 acc_val: 0.7800 time: 0.0029s\n",
      "Epoch: 0120 loss_train: 0.4248 acc_train: 0.9000 loss_val: 0.4316 acc_val: 0.7800 time: 0.0031s\n",
      "Epoch: 0121 loss_train: 0.4247 acc_train: 0.9000 loss_val: 0.4315 acc_val: 0.7800 time: 0.0028s\n",
      "Epoch: 0122 loss_train: 0.4246 acc_train: 0.9000 loss_val: 0.4315 acc_val: 0.7800 time: 0.0029s\n",
      "Epoch: 0123 loss_train: 0.4245 acc_train: 0.9000 loss_val: 0.4315 acc_val: 0.7800 time: 0.0030s\n",
      "Epoch: 0124 loss_train: 0.4244 acc_train: 0.9000 loss_val: 0.4315 acc_val: 0.7800 time: 0.0028s\n",
      "Epoch: 0125 loss_train: 0.4243 acc_train: 0.9000 loss_val: 0.4314 acc_val: 0.7800 time: 0.0029s\n",
      "Epoch: 0126 loss_train: 0.4242 acc_train: 0.9000 loss_val: 0.4313 acc_val: 0.7800 time: 0.0035s\n",
      "Epoch: 0127 loss_train: 0.4241 acc_train: 0.9000 loss_val: 0.4310 acc_val: 0.7800 time: 0.0034s\n",
      "Epoch: 0128 loss_train: 0.4240 acc_train: 0.9000 loss_val: 0.4307 acc_val: 0.7800 time: 0.0035s\n",
      "Epoch: 0129 loss_train: 0.4239 acc_train: 0.9000 loss_val: 0.4306 acc_val: 0.8000 time: 0.0029s\n",
      "Epoch: 0130 loss_train: 0.4238 acc_train: 0.9000 loss_val: 0.4307 acc_val: 0.8000 time: 0.0030s\n",
      "Epoch: 0131 loss_train: 0.4237 acc_train: 0.9000 loss_val: 0.4309 acc_val: 0.8000 time: 0.0031s\n",
      "Epoch: 0132 loss_train: 0.4236 acc_train: 0.9000 loss_val: 0.4311 acc_val: 0.8000 time: 0.0038s\n",
      "Epoch: 0133 loss_train: 0.4235 acc_train: 0.9000 loss_val: 0.4313 acc_val: 0.8000 time: 0.0034s\n",
      "Epoch: 0134 loss_train: 0.4234 acc_train: 0.9000 loss_val: 0.4314 acc_val: 0.8000 time: 0.0037s\n",
      "Epoch: 0135 loss_train: 0.4233 acc_train: 0.9000 loss_val: 0.4314 acc_val: 0.8000 time: 0.0039s\n",
      "Epoch: 0136 loss_train: 0.4232 acc_train: 0.9040 loss_val: 0.4315 acc_val: 0.8000 time: 0.0035s\n",
      "Epoch: 0137 loss_train: 0.4231 acc_train: 0.9040 loss_val: 0.4316 acc_val: 0.8000 time: 0.0036s\n",
      "Epoch: 0138 loss_train: 0.4230 acc_train: 0.9040 loss_val: 0.4317 acc_val: 0.8000 time: 0.0038s\n",
      "Epoch: 0139 loss_train: 0.4229 acc_train: 0.9040 loss_val: 0.4319 acc_val: 0.8000 time: 0.0034s\n",
      "Epoch: 0140 loss_train: 0.4228 acc_train: 0.9040 loss_val: 0.4321 acc_val: 0.8000 time: 0.0033s\n",
      "Epoch: 0141 loss_train: 0.4227 acc_train: 0.9040 loss_val: 0.4322 acc_val: 0.8000 time: 0.0031s\n",
      "Epoch: 0142 loss_train: 0.4226 acc_train: 0.9040 loss_val: 0.4324 acc_val: 0.8000 time: 0.0029s\n",
      "Epoch: 0143 loss_train: 0.4225 acc_train: 0.9040 loss_val: 0.4326 acc_val: 0.8000 time: 0.0030s\n",
      "Epoch: 0144 loss_train: 0.4224 acc_train: 0.9040 loss_val: 0.4328 acc_val: 0.8000 time: 0.0030s\n",
      "Epoch: 0145 loss_train: 0.4223 acc_train: 0.9040 loss_val: 0.4330 acc_val: 0.8000 time: 0.0035s\n",
      "Epoch: 0146 loss_train: 0.4222 acc_train: 0.9040 loss_val: 0.4331 acc_val: 0.8000 time: 0.0037s\n",
      "Epoch: 0147 loss_train: 0.4221 acc_train: 0.9040 loss_val: 0.4333 acc_val: 0.8000 time: 0.0032s\n",
      "Epoch: 0148 loss_train: 0.4220 acc_train: 0.9080 loss_val: 0.4334 acc_val: 0.8000 time: 0.0029s\n",
      "Epoch: 0149 loss_train: 0.4219 acc_train: 0.9080 loss_val: 0.4335 acc_val: 0.8000 time: 0.0027s\n",
      "Epoch: 0150 loss_train: 0.4218 acc_train: 0.9080 loss_val: 0.4337 acc_val: 0.8000 time: 0.0027s\n",
      "Epoch: 0151 loss_train: 0.4217 acc_train: 0.9080 loss_val: 0.4340 acc_val: 0.8000 time: 0.0027s\n",
      "Epoch: 0152 loss_train: 0.4216 acc_train: 0.9080 loss_val: 0.4345 acc_val: 0.8200 time: 0.0031s\n",
      "Epoch: 0153 loss_train: 0.4215 acc_train: 0.9080 loss_val: 0.4351 acc_val: 0.8200 time: 0.0028s\n",
      "Epoch: 0154 loss_train: 0.4214 acc_train: 0.9080 loss_val: 0.4352 acc_val: 0.8200 time: 0.0032s\n",
      "Epoch: 0155 loss_train: 0.4213 acc_train: 0.9080 loss_val: 0.4349 acc_val: 0.8200 time: 0.0031s\n",
      "Epoch: 0156 loss_train: 0.4212 acc_train: 0.9080 loss_val: 0.4348 acc_val: 0.8200 time: 0.0032s\n",
      "Epoch: 0157 loss_train: 0.4211 acc_train: 0.9080 loss_val: 0.4351 acc_val: 0.8200 time: 0.0040s\n",
      "Epoch: 0158 loss_train: 0.4210 acc_train: 0.9080 loss_val: 0.4358 acc_val: 0.8200 time: 0.0028s\n",
      "Epoch: 0159 loss_train: 0.4209 acc_train: 0.9080 loss_val: 0.4368 acc_val: 0.8200 time: 0.0027s\n",
      "Epoch: 0160 loss_train: 0.4208 acc_train: 0.9080 loss_val: 0.4370 acc_val: 0.8200 time: 0.0028s\n",
      "Epoch: 0161 loss_train: 0.4206 acc_train: 0.9080 loss_val: 0.4367 acc_val: 0.8200 time: 0.0031s\n",
      "Epoch: 0162 loss_train: 0.4205 acc_train: 0.9080 loss_val: 0.4367 acc_val: 0.8200 time: 0.0028s\n",
      "Epoch: 0163 loss_train: 0.4203 acc_train: 0.9080 loss_val: 0.4374 acc_val: 0.8200 time: 0.0028s\n",
      "Epoch: 0164 loss_train: 0.4200 acc_train: 0.9080 loss_val: 0.4389 acc_val: 0.8200 time: 0.0028s\n",
      "Epoch: 0165 loss_train: 0.4198 acc_train: 0.9080 loss_val: 0.4403 acc_val: 0.8200 time: 0.0027s\n",
      "Epoch: 0166 loss_train: 0.4195 acc_train: 0.9080 loss_val: 0.4409 acc_val: 0.8200 time: 0.0027s\n",
      "Epoch: 0167 loss_train: 0.4191 acc_train: 0.9080 loss_val: 0.4410 acc_val: 0.8200 time: 0.0028s\n",
      "Epoch: 0168 loss_train: 0.4187 acc_train: 0.9080 loss_val: 0.4422 acc_val: 0.8200 time: 0.0028s\n",
      "Epoch: 0169 loss_train: 0.4183 acc_train: 0.9080 loss_val: 0.4443 acc_val: 0.8200 time: 0.0027s\n",
      "Epoch: 0170 loss_train: 0.4179 acc_train: 0.9160 loss_val: 0.4462 acc_val: 0.8200 time: 0.0028s\n",
      "Epoch: 0171 loss_train: 0.4175 acc_train: 0.9160 loss_val: 0.4470 acc_val: 0.8200 time: 0.0029s\n",
      "Epoch: 0172 loss_train: 0.4171 acc_train: 0.9160 loss_val: 0.4467 acc_val: 0.8200 time: 0.0034s\n",
      "Epoch: 0173 loss_train: 0.4168 acc_train: 0.9160 loss_val: 0.4466 acc_val: 0.8200 time: 0.0030s\n",
      "Epoch: 0174 loss_train: 0.4165 acc_train: 0.9160 loss_val: 0.4475 acc_val: 0.8200 time: 0.0030s\n",
      "Epoch: 0175 loss_train: 0.4163 acc_train: 0.9160 loss_val: 0.4499 acc_val: 0.8200 time: 0.0030s\n",
      "Epoch: 0176 loss_train: 0.4160 acc_train: 0.9160 loss_val: 0.4531 acc_val: 0.8200 time: 0.0034s\n",
      "Epoch: 0177 loss_train: 0.4158 acc_train: 0.9160 loss_val: 0.4561 acc_val: 0.8200 time: 0.0028s\n",
      "Epoch: 0178 loss_train: 0.4156 acc_train: 0.9160 loss_val: 0.4580 acc_val: 0.8400 time: 0.0027s\n",
      "Epoch: 0179 loss_train: 0.4154 acc_train: 0.9160 loss_val: 0.4593 acc_val: 0.8400 time: 0.0029s\n",
      "Epoch: 0180 loss_train: 0.4152 acc_train: 0.9160 loss_val: 0.4610 acc_val: 0.8200 time: 0.0044s\n",
      "Epoch: 0181 loss_train: 0.4151 acc_train: 0.9160 loss_val: 0.4635 acc_val: 0.8200 time: 0.0030s\n",
      "Epoch: 0182 loss_train: 0.4149 acc_train: 0.9160 loss_val: 0.4674 acc_val: 0.8200 time: 0.0028s\n",
      "Epoch: 0183 loss_train: 0.4147 acc_train: 0.9160 loss_val: 0.4719 acc_val: 0.8200 time: 0.0029s\n",
      "Epoch: 0184 loss_train: 0.4145 acc_train: 0.9160 loss_val: 0.4770 acc_val: 0.8200 time: 0.0027s\n",
      "Epoch: 0185 loss_train: 0.4143 acc_train: 0.9160 loss_val: 0.4809 acc_val: 0.8200 time: 0.0028s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0186 loss_train: 0.4141 acc_train: 0.9160 loss_val: 0.4853 acc_val: 0.8200 time: 0.0035s\n",
      "Epoch: 0187 loss_train: 0.4138 acc_train: 0.9160 loss_val: 0.4915 acc_val: 0.8200 time: 0.0032s\n",
      "Epoch: 0188 loss_train: 0.4136 acc_train: 0.9160 loss_val: 0.4990 acc_val: 0.8200 time: 0.0030s\n",
      "Epoch: 0189 loss_train: 0.4134 acc_train: 0.9160 loss_val: 0.5048 acc_val: 0.8200 time: 0.0033s\n",
      "Epoch: 0190 loss_train: 0.4131 acc_train: 0.9200 loss_val: 0.5093 acc_val: 0.8400 time: 0.0033s\n",
      "Epoch: 0191 loss_train: 0.4128 acc_train: 0.9200 loss_val: 0.5148 acc_val: 0.8400 time: 0.0028s\n",
      "Epoch: 0192 loss_train: 0.4125 acc_train: 0.9240 loss_val: 0.5191 acc_val: 0.8400 time: 0.0028s\n",
      "Epoch: 0193 loss_train: 0.4122 acc_train: 0.9240 loss_val: 0.5222 acc_val: 0.8400 time: 0.0032s\n",
      "Epoch: 0194 loss_train: 0.4118 acc_train: 0.9240 loss_val: 0.5242 acc_val: 0.8400 time: 0.0031s\n",
      "Epoch: 0195 loss_train: 0.4115 acc_train: 0.9280 loss_val: 0.5259 acc_val: 0.8400 time: 0.0028s\n",
      "Epoch: 0196 loss_train: 0.4112 acc_train: 0.9280 loss_val: 0.5279 acc_val: 0.8400 time: 0.0028s\n",
      "Epoch: 0197 loss_train: 0.4109 acc_train: 0.9280 loss_val: 0.5298 acc_val: 0.8400 time: 0.0028s\n",
      "Epoch: 0198 loss_train: 0.4107 acc_train: 0.9280 loss_val: 0.5311 acc_val: 0.8400 time: 0.0027s\n",
      "Epoch: 0199 loss_train: 0.4104 acc_train: 0.9280 loss_val: 0.5318 acc_val: 0.8400 time: 0.0028s\n",
      "Epoch: 0200 loss_train: 0.4102 acc_train: 0.9280 loss_val: 0.5323 acc_val: 0.8400 time: 0.0031s\n",
      "Epoch: 0201 loss_train: 0.4101 acc_train: 0.9280 loss_val: 0.5326 acc_val: 0.8400 time: 0.0028s\n",
      "Epoch: 0202 loss_train: 0.4099 acc_train: 0.9280 loss_val: 0.5328 acc_val: 0.8400 time: 0.0027s\n",
      "Epoch: 0203 loss_train: 0.4098 acc_train: 0.9280 loss_val: 0.5327 acc_val: 0.8400 time: 0.0029s\n",
      "Epoch: 0204 loss_train: 0.4096 acc_train: 0.9280 loss_val: 0.5325 acc_val: 0.8600 time: 0.0027s\n",
      "Epoch: 0205 loss_train: 0.4095 acc_train: 0.9280 loss_val: 0.5323 acc_val: 0.8600 time: 0.0028s\n",
      "Epoch: 0206 loss_train: 0.4094 acc_train: 0.9280 loss_val: 0.5322 acc_val: 0.8600 time: 0.0029s\n",
      "Epoch: 0207 loss_train: 0.4093 acc_train: 0.9280 loss_val: 0.5323 acc_val: 0.8600 time: 0.0026s\n",
      "Epoch: 0208 loss_train: 0.4092 acc_train: 0.9280 loss_val: 0.5324 acc_val: 0.8600 time: 0.0028s\n",
      "Epoch: 0209 loss_train: 0.4091 acc_train: 0.9280 loss_val: 0.5325 acc_val: 0.8600 time: 0.0025s\n",
      "Epoch: 0210 loss_train: 0.4090 acc_train: 0.9320 loss_val: 0.5327 acc_val: 0.8600 time: 0.0026s\n",
      "Epoch: 0211 loss_train: 0.4090 acc_train: 0.9320 loss_val: 0.5329 acc_val: 0.8600 time: 0.0028s\n",
      "Epoch: 0212 loss_train: 0.4089 acc_train: 0.9320 loss_val: 0.5332 acc_val: 0.8600 time: 0.0027s\n",
      "Epoch: 0213 loss_train: 0.4088 acc_train: 0.9320 loss_val: 0.5334 acc_val: 0.8600 time: 0.0027s\n",
      "Epoch: 0214 loss_train: 0.4087 acc_train: 0.9320 loss_val: 0.5336 acc_val: 0.8600 time: 0.0028s\n",
      "Epoch: 0215 loss_train: 0.4087 acc_train: 0.9320 loss_val: 0.5337 acc_val: 0.8600 time: 0.0028s\n",
      "Epoch: 0216 loss_train: 0.4086 acc_train: 0.9320 loss_val: 0.5337 acc_val: 0.8600 time: 0.0027s\n",
      "Epoch: 0217 loss_train: 0.4086 acc_train: 0.9320 loss_val: 0.5336 acc_val: 0.8600 time: 0.0025s\n",
      "Epoch: 0218 loss_train: 0.4085 acc_train: 0.9320 loss_val: 0.5336 acc_val: 0.8600 time: 0.0029s\n",
      "Epoch: 0219 loss_train: 0.4084 acc_train: 0.9320 loss_val: 0.5334 acc_val: 0.8600 time: 0.0028s\n",
      "Epoch: 0220 loss_train: 0.4084 acc_train: 0.9320 loss_val: 0.5332 acc_val: 0.8600 time: 0.0029s\n",
      "Epoch: 0221 loss_train: 0.4083 acc_train: 0.9320 loss_val: 0.5330 acc_val: 0.8600 time: 0.0028s\n",
      "Epoch: 0222 loss_train: 0.4083 acc_train: 0.9320 loss_val: 0.5328 acc_val: 0.8600 time: 0.0028s\n",
      "Epoch: 0223 loss_train: 0.4082 acc_train: 0.9320 loss_val: 0.5327 acc_val: 0.8600 time: 0.0027s\n",
      "Epoch: 0224 loss_train: 0.4082 acc_train: 0.9320 loss_val: 0.5326 acc_val: 0.8600 time: 0.0028s\n",
      "Epoch: 0225 loss_train: 0.4081 acc_train: 0.9320 loss_val: 0.5324 acc_val: 0.8600 time: 0.0028s\n",
      "Epoch: 0226 loss_train: 0.4081 acc_train: 0.9320 loss_val: 0.5322 acc_val: 0.8600 time: 0.0029s\n",
      "Epoch: 0227 loss_train: 0.4080 acc_train: 0.9320 loss_val: 0.5320 acc_val: 0.8600 time: 0.0026s\n",
      "Epoch: 0228 loss_train: 0.4080 acc_train: 0.9320 loss_val: 0.5318 acc_val: 0.8600 time: 0.0027s\n",
      "Epoch: 0229 loss_train: 0.4080 acc_train: 0.9320 loss_val: 0.5316 acc_val: 0.8600 time: 0.0026s\n",
      "Epoch: 0230 loss_train: 0.4079 acc_train: 0.9320 loss_val: 0.5315 acc_val: 0.8600 time: 0.0027s\n",
      "Epoch: 0231 loss_train: 0.4079 acc_train: 0.9320 loss_val: 0.5314 acc_val: 0.8600 time: 0.0034s\n",
      "Epoch: 0232 loss_train: 0.4078 acc_train: 0.9320 loss_val: 0.5313 acc_val: 0.8600 time: 0.0029s\n",
      "Epoch: 0233 loss_train: 0.4078 acc_train: 0.9320 loss_val: 0.5313 acc_val: 0.8600 time: 0.0033s\n",
      "Epoch: 0234 loss_train: 0.4078 acc_train: 0.9320 loss_val: 0.5313 acc_val: 0.8600 time: 0.0033s\n",
      "Epoch: 0235 loss_train: 0.4078 acc_train: 0.9320 loss_val: 0.5313 acc_val: 0.8600 time: 0.0027s\n",
      "Epoch: 0236 loss_train: 0.4077 acc_train: 0.9320 loss_val: 0.5313 acc_val: 0.8600 time: 0.0029s\n",
      "Epoch: 0237 loss_train: 0.4077 acc_train: 0.9320 loss_val: 0.5314 acc_val: 0.8600 time: 0.0031s\n",
      "Epoch: 0238 loss_train: 0.4077 acc_train: 0.9320 loss_val: 0.5314 acc_val: 0.8600 time: 0.0028s\n",
      "Epoch: 0239 loss_train: 0.4077 acc_train: 0.9320 loss_val: 0.5315 acc_val: 0.8600 time: 0.0028s\n",
      "Epoch: 0240 loss_train: 0.4076 acc_train: 0.9320 loss_val: 0.5315 acc_val: 0.8600 time: 0.0027s\n",
      "Epoch: 0241 loss_train: 0.4076 acc_train: 0.9320 loss_val: 0.5314 acc_val: 0.8600 time: 0.0028s\n",
      "Epoch: 0242 loss_train: 0.4076 acc_train: 0.9320 loss_val: 0.5313 acc_val: 0.8600 time: 0.0032s\n",
      "Epoch: 0243 loss_train: 0.4076 acc_train: 0.9320 loss_val: 0.5312 acc_val: 0.8600 time: 0.0034s\n",
      "Epoch: 0244 loss_train: 0.4075 acc_train: 0.9320 loss_val: 0.5311 acc_val: 0.8600 time: 0.0040s\n",
      "Epoch: 0245 loss_train: 0.4075 acc_train: 0.9320 loss_val: 0.5309 acc_val: 0.8600 time: 0.0041s\n",
      "Epoch: 0246 loss_train: 0.4075 acc_train: 0.9320 loss_val: 0.5308 acc_val: 0.8600 time: 0.0038s\n",
      "Epoch: 0247 loss_train: 0.4075 acc_train: 0.9320 loss_val: 0.5307 acc_val: 0.8600 time: 0.0034s\n",
      "Epoch: 0248 loss_train: 0.4075 acc_train: 0.9320 loss_val: 0.5307 acc_val: 0.8600 time: 0.0036s\n",
      "Epoch: 0249 loss_train: 0.4074 acc_train: 0.9320 loss_val: 0.5306 acc_val: 0.8600 time: 0.0034s\n",
      "Epoch: 0250 loss_train: 0.4074 acc_train: 0.9320 loss_val: 0.5305 acc_val: 0.8600 time: 0.0036s\n",
      "Epoch: 0251 loss_train: 0.4074 acc_train: 0.9320 loss_val: 0.5304 acc_val: 0.8600 time: 0.0033s\n",
      "Epoch: 0252 loss_train: 0.4074 acc_train: 0.9320 loss_val: 0.5304 acc_val: 0.8600 time: 0.0033s\n",
      "Epoch: 0253 loss_train: 0.4074 acc_train: 0.9320 loss_val: 0.5303 acc_val: 0.8600 time: 0.0038s\n",
      "Epoch: 0254 loss_train: 0.4074 acc_train: 0.9320 loss_val: 0.5302 acc_val: 0.8600 time: 0.0033s\n",
      "Epoch: 0255 loss_train: 0.4073 acc_train: 0.9320 loss_val: 0.5301 acc_val: 0.8600 time: 0.0032s\n",
      "Epoch: 0256 loss_train: 0.4073 acc_train: 0.9320 loss_val: 0.5300 acc_val: 0.8600 time: 0.0031s\n",
      "Epoch: 0257 loss_train: 0.4073 acc_train: 0.9320 loss_val: 0.5299 acc_val: 0.8600 time: 0.0030s\n",
      "Epoch: 0258 loss_train: 0.4073 acc_train: 0.9320 loss_val: 0.5299 acc_val: 0.8600 time: 0.0029s\n",
      "Epoch: 0259 loss_train: 0.4073 acc_train: 0.9320 loss_val: 0.5299 acc_val: 0.8600 time: 0.0028s\n",
      "Epoch: 0260 loss_train: 0.4073 acc_train: 0.9320 loss_val: 0.5298 acc_val: 0.8600 time: 0.0028s\n",
      "Epoch: 0261 loss_train: 0.4073 acc_train: 0.9320 loss_val: 0.5299 acc_val: 0.8600 time: 0.0027s\n",
      "Epoch: 0262 loss_train: 0.4073 acc_train: 0.9320 loss_val: 0.5298 acc_val: 0.8600 time: 0.0028s\n",
      "Epoch: 0263 loss_train: 0.4072 acc_train: 0.9320 loss_val: 0.5298 acc_val: 0.8600 time: 0.0028s\n",
      "Epoch: 0264 loss_train: 0.4072 acc_train: 0.9320 loss_val: 0.5297 acc_val: 0.8600 time: 0.0028s\n",
      "Epoch: 0265 loss_train: 0.4072 acc_train: 0.9320 loss_val: 0.5297 acc_val: 0.8600 time: 0.0028s\n",
      "Epoch: 0266 loss_train: 0.4072 acc_train: 0.9320 loss_val: 0.5296 acc_val: 0.8600 time: 0.0029s\n",
      "Epoch: 0267 loss_train: 0.4072 acc_train: 0.9320 loss_val: 0.5296 acc_val: 0.8600 time: 0.0028s\n",
      "Epoch: 0268 loss_train: 0.4072 acc_train: 0.9320 loss_val: 0.5295 acc_val: 0.8600 time: 0.0029s\n",
      "Epoch: 0269 loss_train: 0.4072 acc_train: 0.9320 loss_val: 0.5294 acc_val: 0.8600 time: 0.0030s\n",
      "Epoch: 0270 loss_train: 0.4071 acc_train: 0.9320 loss_val: 0.5294 acc_val: 0.8600 time: 0.0028s\n",
      "Epoch: 0271 loss_train: 0.4071 acc_train: 0.9320 loss_val: 0.5294 acc_val: 0.8600 time: 0.0030s\n",
      "Epoch: 0272 loss_train: 0.4071 acc_train: 0.9320 loss_val: 0.5293 acc_val: 0.8600 time: 0.0030s\n",
      "Epoch: 0273 loss_train: 0.4071 acc_train: 0.9320 loss_val: 0.5293 acc_val: 0.8600 time: 0.0028s\n",
      "Epoch: 0274 loss_train: 0.4071 acc_train: 0.9320 loss_val: 0.5293 acc_val: 0.8800 time: 0.0031s\n",
      "Epoch: 0275 loss_train: 0.4071 acc_train: 0.9320 loss_val: 0.5293 acc_val: 0.8800 time: 0.0031s\n",
      "Epoch: 0276 loss_train: 0.4071 acc_train: 0.9320 loss_val: 0.5292 acc_val: 0.8800 time: 0.0031s\n",
      "Epoch: 0277 loss_train: 0.4071 acc_train: 0.9320 loss_val: 0.5292 acc_val: 0.8800 time: 0.0031s\n",
      "Epoch: 0278 loss_train: 0.4071 acc_train: 0.9320 loss_val: 0.5291 acc_val: 0.8800 time: 0.0028s\n",
      "Epoch: 0279 loss_train: 0.4070 acc_train: 0.9320 loss_val: 0.5291 acc_val: 0.8800 time: 0.0030s\n",
      "Epoch: 0280 loss_train: 0.4070 acc_train: 0.9320 loss_val: 0.5291 acc_val: 0.8800 time: 0.0028s\n",
      "Epoch: 0281 loss_train: 0.4070 acc_train: 0.9320 loss_val: 0.5291 acc_val: 0.8800 time: 0.0027s\n",
      "Epoch: 0282 loss_train: 0.4070 acc_train: 0.9320 loss_val: 0.5291 acc_val: 0.8800 time: 0.0029s\n",
      "Epoch: 0283 loss_train: 0.4070 acc_train: 0.9320 loss_val: 0.5292 acc_val: 0.8800 time: 0.0031s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0284 loss_train: 0.4070 acc_train: 0.9320 loss_val: 0.5293 acc_val: 0.8800 time: 0.0031s\n",
      "Epoch: 0285 loss_train: 0.4070 acc_train: 0.9320 loss_val: 0.5293 acc_val: 0.8800 time: 0.0029s\n",
      "Epoch: 0286 loss_train: 0.4070 acc_train: 0.9320 loss_val: 0.5293 acc_val: 0.8800 time: 0.0033s\n",
      "Epoch: 0287 loss_train: 0.4070 acc_train: 0.9320 loss_val: 0.5293 acc_val: 0.8800 time: 0.0030s\n",
      "Epoch: 0288 loss_train: 0.4070 acc_train: 0.9320 loss_val: 0.5293 acc_val: 0.8800 time: 0.0035s\n",
      "Epoch: 0289 loss_train: 0.4070 acc_train: 0.9320 loss_val: 0.5293 acc_val: 0.8800 time: 0.0032s\n",
      "Epoch: 0290 loss_train: 0.4069 acc_train: 0.9320 loss_val: 0.5293 acc_val: 0.8800 time: 0.0031s\n",
      "Epoch: 0291 loss_train: 0.4069 acc_train: 0.9320 loss_val: 0.5293 acc_val: 0.8800 time: 0.0033s\n",
      "Epoch: 0292 loss_train: 0.4069 acc_train: 0.9320 loss_val: 0.5293 acc_val: 0.8800 time: 0.0028s\n",
      "Epoch: 0293 loss_train: 0.4069 acc_train: 0.9320 loss_val: 0.5293 acc_val: 0.8800 time: 0.0029s\n",
      "Epoch: 0294 loss_train: 0.4069 acc_train: 0.9320 loss_val: 0.5293 acc_val: 0.8800 time: 0.0039s\n",
      "Epoch: 0295 loss_train: 0.4069 acc_train: 0.9320 loss_val: 0.5292 acc_val: 0.8800 time: 0.0032s\n",
      "Epoch: 0296 loss_train: 0.4069 acc_train: 0.9320 loss_val: 0.5291 acc_val: 0.8800 time: 0.0041s\n",
      "Epoch: 0297 loss_train: 0.4069 acc_train: 0.9320 loss_val: 0.5291 acc_val: 0.8800 time: 0.0035s\n",
      "Epoch: 0298 loss_train: 0.4069 acc_train: 0.9320 loss_val: 0.5290 acc_val: 0.8800 time: 0.0035s\n",
      "Epoch: 0299 loss_train: 0.4069 acc_train: 0.9320 loss_val: 0.5290 acc_val: 0.8800 time: 0.0034s\n",
      "Epoch: 0300 loss_train: 0.4069 acc_train: 0.9320 loss_val: 0.5290 acc_val: 0.8800 time: 0.0031s\n"
     ]
    }
   ],
   "source": [
    "list_loss_t, list_acc_t, list_loss_v, list_acc_v = [], [], [], []\n",
    "for i in range(300):\n",
    "    loss_train, acc_train, loss_val, acc_val = train_v2(i, 0.9999999)\n",
    "    list_loss_t.append(loss_train)\n",
    "    list_acc_t.append(acc_train)\n",
    "    list_loss_v.append(loss_val)\n",
    "    list_acc_v.append(acc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Validation Accuracy')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = range(300)\n",
    "f, axes = plt.subplots(2, 2, figsize = (10,10))\n",
    "axes[0, 0].plot(epochs, list_loss_t)\n",
    "axes[0, 0].set_title(\"Train Loss\")\n",
    "axes[0, 1].plot(epochs, list_acc_t)\n",
    "axes[0, 1].set_title(\"Train Accuracy\")\n",
    "axes[1, 0].plot(epochs, list_loss_v)\n",
    "axes[1, 0].set_title(\"Validation Loss\")\n",
    "axes[1, 1].plot(epochs, list_acc_v)\n",
    "axes[1, 1].set_title(\"Validation Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= -0.8735 accuracy= 0.8736\n"
     ]
    }
   ],
   "source": [
    "# def test():\n",
    "model.eval()\n",
    "output = model(features_1, cm1_lap_4)\n",
    "loss_test = F.nll_loss(output[test_idx_1], labels_1[test_idx_1])\n",
    "acc_test = accuracy(output[test_idx_1], labels_1[test_idx_1])\n",
    "print(\"Test set results:\",\n",
    "      \"loss= {:.4f}\".format(loss_test.item()),\n",
    "      \"accuracy= {:.4f}\".format(acc_test.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(output.argmax(1).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({(0, 0): 332, (1, 0): 29, (1, 1): 21, (0, 1): 5})\n",
      "TN 332\n",
      "FN 5\n",
      "FP 29\n",
      "TP 21\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "c = Counter(sorted(list(zip (output.argmax(1).tolist(),labels_1.tolist()))))\n",
    "\n",
    "arr = np.zeros((2,2))\n",
    "print(c)\n",
    "for (x,y),count in c.items():\n",
    "    arr[x,y] =count\n",
    "    print({(0,0):\"TN\",(0,1):\"FN\",(1,0):\"FP\",(1,1):\"TP\"}[(x,y)],count)\n",
    "    \n",
    "plt.imshow(arr)\n",
    "for (x,y),count in c.items():\n",
    "    plt.text(x,y,count,c=\"w\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision is 0.42\n"
     ]
    }
   ],
   "source": [
    "precision = c[(1,1)] / (c[(1,0)] + c[(1,1)])\n",
    "print(\"Precision is {}\".format(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fpr, tpr, thresholds = sklearn.metrics.roc_curve(labels_1.cpu().tolist(), output[:,1].cpu().tolist())\n",
    "\n",
    "auc = sklearn.metrics.auc(fpr, tpr)\n",
    "\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %.2f)'%auc)\n",
    "plt.legend()\n",
    "plt.title('ROC curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_k1 = GCN(nfeat = features.shape[1],\n",
    "            nhid1 = 16,\n",
    "            nhid2 = 16,\n",
    "            nhid3 = 8,\n",
    "            nclass= 2,\n",
    "            dropout = 0.5)\n",
    "optimizer_k1 = optim.Adam(model_k1.parameters(),\n",
    "                       lr = 0.01, weight_decay = 5e-4)\n",
    "\n",
    "model_k2 = GCN(nfeat = features.shape[1],\n",
    "            nhid1 = 16,\n",
    "            nhid2 = 16,\n",
    "            nhid3 = 8,\n",
    "            nclass= 2,\n",
    "            dropout = 0.5)\n",
    "optimizer_k2 = optim.Adam(model_k2.parameters(),\n",
    "                       lr = 0.01, weight_decay = 5e-4)\n",
    "\n",
    "\n",
    "model_k3 = GCN(nfeat = features.shape[1],\n",
    "            nhid1 = 16,\n",
    "            nhid2 = 16,\n",
    "            nhid3 = 8,\n",
    "            nclass= 2,\n",
    "            dropout = 0.5)\n",
    "optimizer_k3 = optim.Adam(model_k3.parameters(),\n",
    "                       lr = 0.01, weight_decay = 5e-4)\n",
    "\n",
    "model_k4 = GCN(nfeat = features.shape[1],\n",
    "            nhid1 = 16,\n",
    "            nhid2 = 16,\n",
    "            nhid3 = 8,\n",
    "            nclass= 2,\n",
    "            dropout = 0.5)\n",
    "optimizer_k4 = optim.Adam(model_k4.parameters(),\n",
    "                       lr = 0.01, weight_decay = 5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_k1(epoch, beta):\n",
    "    ######calculate the weights based on the paper with effective number######\n",
    "    no_of_classes = 2\n",
    "    effective_num = 1.0 - np.power(beta, classes_sizes)\n",
    "    weights = (1.0 - beta) / np.array(effective_num)\n",
    "    weights = weights / np.sum(weights) * no_of_classes\n",
    "    l = torch.tensor([0, 1])\n",
    "    labels_one_hot = F.one_hot(l, no_of_classes).float()\n",
    "    weights = torch.tensor(weights).float()\n",
    "    if epoch == 0: print(weights)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(weight = weights)\n",
    "    ###########################################################\n",
    "    \n",
    "    t = time.time()\n",
    "\n",
    "    model_k1.train()\n",
    "    optimizer_k1.zero_grad()\n",
    "    output = model_k1(features_1, norm_lap_1)\n",
    "    \n",
    "    loss_train = criterion(output[train_idx_1], labels_1[train_idx_1])\n",
    "    acc_train = accuracy(output[train_idx_1], labels_1[train_idx_1])\n",
    "    optimizer_k1.zero_grad()\n",
    "    loss_train.backward()\n",
    "    optimizer_k1.step()\n",
    "\n",
    "    loss_val = criterion(output[val_idx_1], labels_1[val_idx_1])\n",
    "    acc_val = accuracy(output[val_idx_1], labels_1[val_idx_1])\n",
    "    \n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "    \n",
    "    return loss_train, acc_train, loss_val, acc_val\n",
    "\n",
    "def train_k2(epoch, beta):\n",
    "    ######calculate the weights based on the paper with effective number######\n",
    "    no_of_classes = 2\n",
    "    effective_num = 1.0 - np.power(beta, classes_sizes)\n",
    "    weights = (1.0 - beta) / np.array(effective_num)\n",
    "    weights = weights / np.sum(weights) * no_of_classes\n",
    "    l = torch.tensor([0, 1])\n",
    "    labels_one_hot = F.one_hot(l, no_of_classes).float()\n",
    "    weights = torch.tensor(weights).float()\n",
    "    if epoch == 0: print(weights)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(weight = weights)\n",
    "    ###########################################################\n",
    "    \n",
    "    t = time.time()\n",
    "\n",
    "    model_k2.train()\n",
    "    optimizer_k2.zero_grad()\n",
    "    output = model_k2(features_1, k2_lap_1)\n",
    "    \n",
    "    loss_train = criterion(output[train_idx_1], labels_1[train_idx_1])\n",
    "    acc_train = accuracy(output[train_idx_1], labels_1[train_idx_1])\n",
    "    optimizer_k2.zero_grad()\n",
    "    loss_train.backward()\n",
    "    optimizer_k2.step()\n",
    "\n",
    "    loss_val = criterion(output[val_idx_1], labels_1[val_idx_1])\n",
    "    acc_val = accuracy(output[val_idx_1], labels_1[val_idx_1])\n",
    "    \n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "    \n",
    "    return loss_train, acc_train, loss_val, acc_val\n",
    "\n",
    "def train_k3(epoch, beta):\n",
    "    ######calculate the weights based on the paper with effective number######\n",
    "    no_of_classes = 2\n",
    "    effective_num = 1.0 - np.power(beta, classes_sizes)\n",
    "    weights = (1.0 - beta) / np.array(effective_num)\n",
    "    weights = weights / np.sum(weights) * no_of_classes\n",
    "    l = torch.tensor([0, 1])\n",
    "    labels_one_hot = F.one_hot(l, no_of_classes).float()\n",
    "    weights = torch.tensor(weights).float()\n",
    "    if epoch == 0: print(weights)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(weight = weights)\n",
    "    ###########################################################\n",
    "    \n",
    "    t = time.time()\n",
    "\n",
    "    model_k3.train()\n",
    "    optimizer_k3.zero_grad()\n",
    "    output = model_k3(features_1, k3_lap_1)\n",
    "    \n",
    "    loss_train = criterion(output[train_idx_1], labels_1[train_idx_1])\n",
    "    acc_train = accuracy(output[train_idx_1], labels_1[train_idx_1])\n",
    "    optimizer_k3.zero_grad()\n",
    "    loss_train.backward()\n",
    "    optimizer_k3.step()\n",
    "\n",
    "    loss_val = criterion(output[val_idx_1], labels_1[val_idx_1])\n",
    "    acc_val = accuracy(output[val_idx_1], labels_1[val_idx_1])\n",
    "    \n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "    \n",
    "    return loss_train, acc_train, loss_val, acc_val\n",
    "\n",
    "def train_k4(epoch, beta):\n",
    "    ######calculate the weights based on the paper with effective number######\n",
    "    no_of_classes = 2\n",
    "    effective_num = 1.0 - np.power(beta, classes_sizes)\n",
    "    weights = (1.0 - beta) / np.array(effective_num)\n",
    "    weights = weights / np.sum(weights) * no_of_classes\n",
    "    l = torch.tensor([0, 1])\n",
    "    labels_one_hot = F.one_hot(l, no_of_classes).float()\n",
    "    weights = torch.tensor(weights).float()\n",
    "    if epoch == 0: print(weights)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(weight = weights)\n",
    "    ###########################################################\n",
    "    \n",
    "    t = time.time()\n",
    "\n",
    "    model_k4.train()\n",
    "    optimizer_k4.zero_grad()\n",
    "    output = model_k4(features_1, k4_lap_1)\n",
    "    \n",
    "    loss_train = criterion(output[train_idx_1], labels_1[train_idx_1])\n",
    "    acc_train = accuracy(output[train_idx_1], labels_1[train_idx_1])\n",
    "    optimizer_k4.zero_grad()\n",
    "    loss_train.backward()\n",
    "    optimizer_k4.step()\n",
    "\n",
    "    loss_val = criterion(output[val_idx_1], labels_1[val_idx_1])\n",
    "    acc_val = accuracy(output[val_idx_1], labels_1[val_idx_1])\n",
    "    \n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "    \n",
    "    return loss_train, acc_train, loss_val, acc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_loss_t_k1, list_acc_t_k1, list_loss_v_k1, list_acc_v_k1 = [], [], [], []\n",
    "for i in range(300):\n",
    "    loss_train_k1, acc_train_k1, loss_val_k1, acc_val_k1 = train_k1(i, 0.9999999)\n",
    "    list_loss_t_k1.append(loss_train_k1)\n",
    "    list_acc_t_k1.append(acc_train_k1)\n",
    "    list_loss_v_k1.append(loss_val_k1)\n",
    "    list_acc_v_k1.append(acc_val_k1)\n",
    "\n",
    "list_loss_t_k2, list_acc_t_k2, list_loss_v_k2, list_acc_v_k2 = [], [], [], []\n",
    "for i in range(300):\n",
    "    loss_train_k2, acc_train_k2, loss_val_k2, acc_val_k2 = train_k2(i, 0.9999999)\n",
    "    list_loss_t_k2.append(loss_train_k2)\n",
    "    list_acc_t_k2.append(acc_train_k2)\n",
    "    list_loss_v_k2.append(loss_val_k2)\n",
    "    list_acc_v_k2.append(acc_val_k2)\n",
    "    \n",
    "list_loss_t_k3, list_acc_t_k3, list_loss_v_k3, list_acc_v_k3 = [], [], [], []\n",
    "for i in range(300):\n",
    "    loss_train_k3, acc_train_k3, loss_val_k3, acc_val_k3 = train_k3(i, 0.9999999)\n",
    "    list_loss_t_k3.append(loss_train_k3)\n",
    "    list_acc_t_k3.append(acc_train_k3)\n",
    "    list_loss_v_k3.append(loss_val_k3)\n",
    "    list_acc_v_k3.append(acc_val_k3)\n",
    "    \n",
    "list_loss_t_k4, list_acc_t_k4, list_loss_v_k4, list_acc_v_k4 = [], [], [], []\n",
    "for i in range(300):\n",
    "    loss_train_k4, acc_train_k4, loss_val_k4, acc_val_k4 = train_k4(i, 0.9999999)\n",
    "    list_loss_t_k4.append(loss_train_k4)\n",
    "    list_acc_t_k4.append(acc_train_k4)\n",
    "    list_loss_v_k4.append(loss_val_k4)\n",
    "    list_acc_v_k4.append(acc_val_k4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(300)\n",
    "f, axes = plt.subplots(2, 2, figsize = (10,10))\n",
    "axes[0, 0].plot(epochs, list_loss_t_k1, 'r')\n",
    "axes[0, 0].plot(epochs, list_loss_t_k2, 'g')\n",
    "axes[0, 0].plot(epochs, list_loss_t_k3, 'b')\n",
    "axes[0, 0].plot(epochs, list_loss_t_k4, 'y')\n",
    "axes[0, 0].set_title(\"Train Loss\")\n",
    "\n",
    "axes[0, 1].plot(epochs, list_acc_t_k1, 'r')\n",
    "axes[0, 1].plot(epochs, list_acc_t_k2, 'g')\n",
    "axes[0, 1].plot(epochs, list_acc_t_k3, 'b')\n",
    "axes[0, 1].plot(epochs, list_acc_t_k4, 'y')\n",
    "axes[0, 1].set_title(\"Train Accuracy\")\n",
    "\n",
    "axes[1, 0].plot(epochs, list_loss_v_k1, 'r')\n",
    "axes[1, 0].plot(epochs, list_loss_v_k2, 'g')\n",
    "axes[1, 0].plot(epochs, list_loss_v_k3, 'b')\n",
    "axes[1, 0].plot(epochs, list_loss_v_k4, 'y')\n",
    "axes[1, 0].set_title(\"Validation Loss\")\n",
    "\n",
    "axes[1, 1].plot(epochs, list_acc_v_k1, 'r')\n",
    "axes[1, 1].plot(epochs, list_acc_v_k2, 'g')\n",
    "axes[1, 1].plot(epochs, list_acc_v_k3, 'b')\n",
    "axes[1, 1].plot(epochs, list_acc_v_k4, 'y')\n",
    "axes[1, 1].set_title(\"Validation Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_k1.eval()\n",
    "output_k1 = model_k1(features_1, norm_lap_1)\n",
    "loss_test = F.nll_loss(output_k1[test_idx_1], labels_1[test_idx_1])\n",
    "acc_test = accuracy(output_k1[test_idx_1], labels_1[test_idx_1])\n",
    "print(\"Test set results:\",\n",
    "      \"loss= {:.4f}\".format(loss_test.item()),\n",
    "      \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "\n",
    "str(output_k1.argmax(1).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_k2.eval()\n",
    "output_k2 = model_k2(features_1, k2_lap_1)\n",
    "loss_test = F.nll_loss(output_k2[test_idx_1], labels_1[test_idx_1])\n",
    "acc_test = accuracy(output_k2[test_idx_1], labels_1[test_idx_1])\n",
    "print(\"Test set results:\",\n",
    "      \"loss= {:.4f}\".format(loss_test.item()),\n",
    "      \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "\n",
    "str(output_k2.argmax(1).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_k3.eval()\n",
    "output_k3 = model_k3(features_1, k3_lap_1)\n",
    "loss_test = F.nll_loss(output_k3[test_idx_1], labels_1[test_idx_1])\n",
    "acc_test = accuracy(output_k3[test_idx_1], labels_1[test_idx_1])\n",
    "print(\"Test set results:\",\n",
    "      \"loss= {:.4f}\".format(loss_test.item()),\n",
    "      \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "\n",
    "str(output_k3.argmax(1).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_k4.eval()\n",
    "output_k4 = model_k4(features_1, k4_lap_1)\n",
    "loss_test = F.nll_loss(output_k4[test_idx_1], labels_1[test_idx_1])\n",
    "acc_test = accuracy(output_k4[test_idx_1], labels_1[test_idx_1])\n",
    "print(\"Test set results:\",\n",
    "      \"loss= {:.4f}\".format(loss_test.item()),\n",
    "      \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "\n",
    "str(output_k4.argmax(1).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_k1, tpr_k1, thresholds_k1 = sklearn.metrics.roc_curve(labels_1[test_idx_1].cpu().tolist(), output_k1[test_idx_1][:,1].cpu().tolist())\n",
    "auc_k1 = sklearn.metrics.auc(fpr_k1, tpr_k1)\n",
    "\n",
    "fpr_k2, tpr_k2, thresholds_k2 = sklearn.metrics.roc_curve(labels_1[test_idx_1].cpu().tolist(), output_k2[test_idx_1][:,1].cpu().tolist())\n",
    "auc_k2 = sklearn.metrics.auc(fpr_k2, tpr_k2)\n",
    "\n",
    "fpr_k3, tpr_k3, thresholds_k3 = sklearn.metrics.roc_curve(labels_1[test_idx_1].cpu().tolist(), output_k3[test_idx_1][:,1].cpu().tolist())\n",
    "auc_k3 = sklearn.metrics.auc(fpr_k3, tpr_k3)\n",
    "\n",
    "fpr_k4, tpr_k4, thresholds_k4 = sklearn.metrics.roc_curve(labels_1[test_idx_1].cpu().tolist(), output_k4[test_idx_1][:,1].cpu().tolist())\n",
    "auc_k4 = sklearn.metrics.auc(fpr_k4, tpr_k4)\n",
    "\n",
    "plt.plot(fpr_k1, tpr_k1, label='K1 (area = %.2f)'%auc_k1)\n",
    "plt.plot(fpr_k2, tpr_k2, label='K2 (area = %.2f)'%auc_k2)\n",
    "plt.plot(fpr_k3, tpr_k3, label='K3 (area = %.2f)'%auc_k3)\n",
    "plt.plot(fpr_k4, tpr_k4, label='K4 (area = %.2f)'%auc_k4)\n",
    "plt.legend()\n",
    "plt.title('ROC curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_k1, tpr_k1, thresholds_k1 = sklearn.metrics.roc_curve(labels_1.cpu().tolist(), output_k1[:,1].cpu().tolist())\n",
    "auc_k1 = sklearn.metrics.auc(fpr_k1, tpr_k1)\n",
    "\n",
    "fpr_k2, tpr_k2, thresholds_k2 = sklearn.metrics.roc_curve(labels_1.cpu().tolist(), output_k2[:,1].cpu().tolist())\n",
    "auc_k2 = sklearn.metrics.auc(fpr_k2, tpr_k2)\n",
    "\n",
    "fpr_k3, tpr_k3, thresholds_k3 = sklearn.metrics.roc_curve(labels_1.cpu().tolist(), output_k3[:,1].cpu().tolist())\n",
    "auc_k3 = sklearn.metrics.auc(fpr_k3, tpr_k3)\n",
    "\n",
    "fpr_k4, tpr_k4, thresholds_k4 = sklearn.metrics.roc_curve(labels_1.cpu().tolist(), output_k4[:,1].cpu().tolist())\n",
    "auc_k4 = sklearn.metrics.auc(fpr_k4, tpr_k4)\n",
    "\n",
    "plt.plot(fpr_k1, tpr_k1, label='K1 (area = %.2f)'%auc_k1)\n",
    "plt.plot(fpr_k2, tpr_k2, label='K2 (area = %.2f)'%auc_k2)\n",
    "plt.plot(fpr_k3, tpr_k3, label='K3 (area = %.2f)'%auc_k3)\n",
    "plt.plot(fpr_k4, tpr_k4, label='K4 (area = %.2f)'%auc_k4)\n",
    "plt.legend()\n",
    "plt.title('ROC curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp = output_k4.tolist()\n",
    "print(np.where(temp[:][0] == temp[:][1]))\n",
    "print(\"No equal problilities for the two class in the given result k4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
